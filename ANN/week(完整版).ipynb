{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as mat\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from keras import layers, optimizers, models\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取資料並做資料前處理將特殊字元處理掉\n",
    "train_data =pd.read_csv('adult.data',sep=\" \", header=None)\n",
    "train_data = train_data.replace({'\\$': '', ',': '','\\.':'','<=50K':'1','>50K':'0'}, regex=True) #砍掉換行&逗號\n",
    "train_data.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','label']\n",
    "\n",
    "test_data =pd.read_csv('adult.test',sep=\" \", header=None)\n",
    "test_data = test_data.replace({'\\$': '', ',': '','<=50K.':'1','>50K.':'0'}, regex=True) #砍掉換行&逗號\n",
    "test_data.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合併train data和test data\n",
    "data_1 = train_data\n",
    "data_1=data_1.append(test_data)\n",
    "#data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將缺值欄位補上平均值\n",
    "data_1.replace('?', np.nan, inplace=True)\n",
    "data_1=data_1.fillna(data_1.mean())\n",
    "data_1 = data_1.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "#data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將資料轉成int\n",
    "data_1[['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week','label'] ]=data_1[['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week','label']].astype(str).astype(int)\n",
    "#將文字資料(要做one hot encoding和label切出)\n",
    "data_cat = data_1[['workclass','education','marital-status','occupation','relationship','race','sex','native-country']]\n",
    "data_hours = data_1[['hours-per-week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "data_cat = pd.get_dummies(data_cat)\n",
    "#data_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdata = data_1.drop(['hours-per-week','workclass','education','marital-status','occupation','relationship','race','sex','native-country'],axis=1).join(data_cat,how='left')\n",
    "#newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將原始資料合併one hot encoding後的資料\n",
    "newdata1 = data_1.drop(['workclass','education','marital-status','occupation','relationship','race','sex','native-country'],axis=1)\n",
    "newdata_merge = pd.concat([newdata1,data_cat],axis=1).reindex(data_1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdata_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>label</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.044131</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.02174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.048052</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.137581</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.150486</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.220635</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "0  0.301370  0.044131       0.800000       0.02174           0.0   \n",
       "1  0.452055  0.048052       0.800000       0.00000           0.0   \n",
       "2  0.287671  0.137581       0.533333       0.00000           0.0   \n",
       "3  0.493151  0.150486       0.400000       0.00000           0.0   \n",
       "4  0.150685  0.220635       0.800000       0.00000           0.0   \n",
       "\n",
       "   hours-per-week  label  workclass_Federal-gov  workclass_Local-gov  \\\n",
       "0        0.397959    1.0                    0.0                  0.0   \n",
       "1        0.122449    1.0                    0.0                  0.0   \n",
       "2        0.397959    1.0                    0.0                  0.0   \n",
       "3        0.397959    1.0                    0.0                  0.0   \n",
       "4        0.397959    1.0                    0.0                  0.0   \n",
       "\n",
       "   workclass_Never-worked  ...  native-country_Portugal  \\\n",
       "0                     0.0  ...                      0.0   \n",
       "1                     0.0  ...                      0.0   \n",
       "2                     0.0  ...                      0.0   \n",
       "3                     0.0  ...                      0.0   \n",
       "4                     0.0  ...                      0.0   \n",
       "\n",
       "   native-country_Puerto-Rico  native-country_Scotland  native-country_South  \\\n",
       "0                         0.0                      0.0                   0.0   \n",
       "1                         0.0                      0.0                   0.0   \n",
       "2                         0.0                      0.0                   0.0   \n",
       "3                         0.0                      0.0                   0.0   \n",
       "4                         0.0                      0.0                   0.0   \n",
       "\n",
       "   native-country_Taiwan  native-country_Thailand  \\\n",
       "0                    0.0                      0.0   \n",
       "1                    0.0                      0.0   \n",
       "2                    0.0                      0.0   \n",
       "3                    0.0                      0.0   \n",
       "4                    0.0                      0.0   \n",
       "\n",
       "   native-country_Trinadad&Tobago  native-country_United-States  \\\n",
       "0                             0.0                           1.0   \n",
       "1                             0.0                           1.0   \n",
       "2                             0.0                           1.0   \n",
       "3                             0.0                           1.0   \n",
       "4                             0.0                           0.0   \n",
       "\n",
       "   native-country_Vietnam  native-country_Yugoslavia  \n",
       "0                     0.0                        0.0  \n",
       "1                     0.0                        0.0  \n",
       "2                     0.0                        0.0  \n",
       "3                     0.0                        0.0  \n",
       "4                     0.0                        0.0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#將資料正規化\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(newdata_merge)\n",
    "data_norm= pd.DataFrame(np_scaled, columns = newdata_merge.columns)\n",
    "data_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data=newdata_merge.iloc[32561:]\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data=newdata_merge.iloc[:32561]\n",
    "#train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>label</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32561</th>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.145129</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32562</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.052451</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32563</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.219649</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32564</th>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.100153</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.076881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32565</th>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.061708</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.295918</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.137428</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>0.643836</td>\n",
       "      <td>0.209130</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.245379</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.048444</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.054551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>0.246575</td>\n",
       "      <td>0.114919</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16281 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "32561  0.109589  0.145129       0.400000      0.000000           0.0   \n",
       "32562  0.287671  0.052451       0.533333      0.000000           0.0   \n",
       "32563  0.150685  0.219649       0.733333      0.000000           0.0   \n",
       "32564  0.369863  0.100153       0.600000      0.076881           0.0   \n",
       "32565  0.013699  0.061708       0.600000      0.000000           0.0   \n",
       "...         ...       ...            ...           ...           ...   \n",
       "48837  0.301370  0.137428       0.800000      0.000000           0.0   \n",
       "48838  0.643836  0.209130       0.533333      0.000000           0.0   \n",
       "48839  0.287671  0.245379       0.800000      0.000000           0.0   \n",
       "48840  0.369863  0.048444       0.800000      0.054551           0.0   \n",
       "48841  0.246575  0.114919       0.800000      0.000000           0.0   \n",
       "\n",
       "       hours-per-week  label  workclass_Federal-gov  workclass_Local-gov  \\\n",
       "32561        0.397959    1.0                    0.0                  0.0   \n",
       "32562        0.500000    1.0                    0.0                  0.0   \n",
       "32563        0.397959    0.0                    0.0                  1.0   \n",
       "32564        0.397959    0.0                    0.0                  0.0   \n",
       "32565        0.295918    1.0                    0.0                  0.0   \n",
       "...               ...    ...                    ...                  ...   \n",
       "48837        0.357143    1.0                    0.0                  0.0   \n",
       "48838        0.397959    1.0                    0.0                  0.0   \n",
       "48839        0.500000    1.0                    0.0                  0.0   \n",
       "48840        0.397959    1.0                    0.0                  0.0   \n",
       "48841        0.602041    0.0                    0.0                  0.0   \n",
       "\n",
       "       workclass_Never-worked  ...  native-country_Portugal  \\\n",
       "32561                     0.0  ...                      0.0   \n",
       "32562                     0.0  ...                      0.0   \n",
       "32563                     0.0  ...                      0.0   \n",
       "32564                     0.0  ...                      0.0   \n",
       "32565                     0.0  ...                      0.0   \n",
       "...                       ...  ...                      ...   \n",
       "48837                     0.0  ...                      0.0   \n",
       "48838                     0.0  ...                      0.0   \n",
       "48839                     0.0  ...                      0.0   \n",
       "48840                     0.0  ...                      0.0   \n",
       "48841                     0.0  ...                      0.0   \n",
       "\n",
       "       native-country_Puerto-Rico  native-country_Scotland  \\\n",
       "32561                         0.0                      0.0   \n",
       "32562                         0.0                      0.0   \n",
       "32563                         0.0                      0.0   \n",
       "32564                         0.0                      0.0   \n",
       "32565                         0.0                      0.0   \n",
       "...                           ...                      ...   \n",
       "48837                         0.0                      0.0   \n",
       "48838                         0.0                      0.0   \n",
       "48839                         0.0                      0.0   \n",
       "48840                         0.0                      0.0   \n",
       "48841                         0.0                      0.0   \n",
       "\n",
       "       native-country_South  native-country_Taiwan  native-country_Thailand  \\\n",
       "32561                   0.0                    0.0                      0.0   \n",
       "32562                   0.0                    0.0                      0.0   \n",
       "32563                   0.0                    0.0                      0.0   \n",
       "32564                   0.0                    0.0                      0.0   \n",
       "32565                   0.0                    0.0                      0.0   \n",
       "...                     ...                    ...                      ...   \n",
       "48837                   0.0                    0.0                      0.0   \n",
       "48838                   0.0                    0.0                      0.0   \n",
       "48839                   0.0                    0.0                      0.0   \n",
       "48840                   0.0                    0.0                      0.0   \n",
       "48841                   0.0                    0.0                      0.0   \n",
       "\n",
       "       native-country_Trinadad&Tobago  native-country_United-States  \\\n",
       "32561                             0.0                           1.0   \n",
       "32562                             0.0                           1.0   \n",
       "32563                             0.0                           1.0   \n",
       "32564                             0.0                           1.0   \n",
       "32565                             0.0                           1.0   \n",
       "...                               ...                           ...   \n",
       "48837                             0.0                           1.0   \n",
       "48838                             0.0                           1.0   \n",
       "48839                             0.0                           1.0   \n",
       "48840                             0.0                           1.0   \n",
       "48841                             0.0                           1.0   \n",
       "\n",
       "       native-country_Vietnam  native-country_Yugoslavia  \n",
       "32561                     0.0                        0.0  \n",
       "32562                     0.0                        0.0  \n",
       "32563                     0.0                        0.0  \n",
       "32564                     0.0                        0.0  \n",
       "32565                     0.0                        0.0  \n",
       "...                       ...                        ...  \n",
       "48837                     0.0                        0.0  \n",
       "48838                     0.0                        0.0  \n",
       "48839                     0.0                        0.0  \n",
       "48840                     0.0                        0.0  \n",
       "48841                     0.0                        0.0  \n",
       "\n",
       "[16281 rows x 106 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data1=data_norm.iloc[32561:] #分解train data和test data\n",
    "test_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>label</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.044131</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.048052</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.137581</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.150486</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.220635</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.165763</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>0.315068</td>\n",
       "      <td>0.096129</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>0.561644</td>\n",
       "      <td>0.094462</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.128004</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.186482</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.150242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "0      0.301370  0.044131       0.800000      0.021740           0.0   \n",
       "1      0.452055  0.048052       0.800000      0.000000           0.0   \n",
       "2      0.287671  0.137581       0.533333      0.000000           0.0   \n",
       "3      0.493151  0.150486       0.400000      0.000000           0.0   \n",
       "4      0.150685  0.220635       0.800000      0.000000           0.0   \n",
       "...         ...       ...            ...           ...           ...   \n",
       "32556  0.136986  0.165763       0.733333      0.000000           0.0   \n",
       "32557  0.315068  0.096129       0.533333      0.000000           0.0   \n",
       "32558  0.561644  0.094462       0.533333      0.000000           0.0   \n",
       "32559  0.068493  0.128004       0.533333      0.000000           0.0   \n",
       "32560  0.479452  0.186482       0.533333      0.150242           0.0   \n",
       "\n",
       "       hours-per-week  label  workclass_Federal-gov  workclass_Local-gov  \\\n",
       "0            0.397959    1.0                    0.0                  0.0   \n",
       "1            0.122449    1.0                    0.0                  0.0   \n",
       "2            0.397959    1.0                    0.0                  0.0   \n",
       "3            0.397959    1.0                    0.0                  0.0   \n",
       "4            0.397959    1.0                    0.0                  0.0   \n",
       "...               ...    ...                    ...                  ...   \n",
       "32556        0.377551    1.0                    0.0                  0.0   \n",
       "32557        0.397959    0.0                    0.0                  0.0   \n",
       "32558        0.397959    1.0                    0.0                  0.0   \n",
       "32559        0.193878    1.0                    0.0                  0.0   \n",
       "32560        0.397959    0.0                    0.0                  0.0   \n",
       "\n",
       "       workclass_Never-worked  ...  native-country_Portugal  \\\n",
       "0                         0.0  ...                      0.0   \n",
       "1                         0.0  ...                      0.0   \n",
       "2                         0.0  ...                      0.0   \n",
       "3                         0.0  ...                      0.0   \n",
       "4                         0.0  ...                      0.0   \n",
       "...                       ...  ...                      ...   \n",
       "32556                     0.0  ...                      0.0   \n",
       "32557                     0.0  ...                      0.0   \n",
       "32558                     0.0  ...                      0.0   \n",
       "32559                     0.0  ...                      0.0   \n",
       "32560                     0.0  ...                      0.0   \n",
       "\n",
       "       native-country_Puerto-Rico  native-country_Scotland  \\\n",
       "0                             0.0                      0.0   \n",
       "1                             0.0                      0.0   \n",
       "2                             0.0                      0.0   \n",
       "3                             0.0                      0.0   \n",
       "4                             0.0                      0.0   \n",
       "...                           ...                      ...   \n",
       "32556                         0.0                      0.0   \n",
       "32557                         0.0                      0.0   \n",
       "32558                         0.0                      0.0   \n",
       "32559                         0.0                      0.0   \n",
       "32560                         0.0                      0.0   \n",
       "\n",
       "       native-country_South  native-country_Taiwan  native-country_Thailand  \\\n",
       "0                       0.0                    0.0                      0.0   \n",
       "1                       0.0                    0.0                      0.0   \n",
       "2                       0.0                    0.0                      0.0   \n",
       "3                       0.0                    0.0                      0.0   \n",
       "4                       0.0                    0.0                      0.0   \n",
       "...                     ...                    ...                      ...   \n",
       "32556                   0.0                    0.0                      0.0   \n",
       "32557                   0.0                    0.0                      0.0   \n",
       "32558                   0.0                    0.0                      0.0   \n",
       "32559                   0.0                    0.0                      0.0   \n",
       "32560                   0.0                    0.0                      0.0   \n",
       "\n",
       "       native-country_Trinadad&Tobago  native-country_United-States  \\\n",
       "0                                 0.0                           1.0   \n",
       "1                                 0.0                           1.0   \n",
       "2                                 0.0                           1.0   \n",
       "3                                 0.0                           1.0   \n",
       "4                                 0.0                           0.0   \n",
       "...                               ...                           ...   \n",
       "32556                             0.0                           1.0   \n",
       "32557                             0.0                           1.0   \n",
       "32558                             0.0                           1.0   \n",
       "32559                             0.0                           1.0   \n",
       "32560                             0.0                           1.0   \n",
       "\n",
       "       native-country_Vietnam  native-country_Yugoslavia  \n",
       "0                         0.0                        0.0  \n",
       "1                         0.0                        0.0  \n",
       "2                         0.0                        0.0  \n",
       "3                         0.0                        0.0  \n",
       "4                         0.0                        0.0  \n",
       "...                       ...                        ...  \n",
       "32556                     0.0                        0.0  \n",
       "32557                     0.0                        0.0  \n",
       "32558                     0.0                        0.0  \n",
       "32559                     0.0                        0.0  \n",
       "32560                     0.0                        0.0  \n",
       "\n",
       "[32561 rows x 106 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data1=data_norm.iloc[:32561] #分解train data和test data\n",
    "train_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_true=data_hours.iloc[:32561] #擷取traindata label\n",
    "#train_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true=data_hours.iloc[32561:] #擷取testdata label\n",
    "#test_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, optimizers, models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train1 = train_data1.drop('hours-per-week', axis=1)\n",
    "y_train1 = train_true\n",
    "\n",
    "X_test1 = test_data1.drop('hours-per-week', axis=1)\n",
    "y_test1 = test_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26048 samples, validate on 6513 samples\n",
      "Epoch 1/1000000\n",
      "26048/26048 [==============================] - 0s 16us/step - loss: 1198.4975 - accuracy: 0.0046 - val_loss: 229.1376 - val_accuracy: 0.0123\n",
      "Epoch 2/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 335.6535 - accuracy: 0.0204 - val_loss: 156.8854 - val_accuracy: 0.0613\n",
      "Epoch 3/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 300.7744 - accuracy: 0.0238 - val_loss: 149.0055 - val_accuracy: 0.0649\n",
      "Epoch 4/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 283.9771 - accuracy: 0.0230 - val_loss: 145.1805 - val_accuracy: 0.0680\n",
      "Epoch 5/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 277.4481 - accuracy: 0.0235 - val_loss: 143.3571 - val_accuracy: 0.0665\n",
      "Epoch 6/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 274.4005 - accuracy: 0.0249 - val_loss: 144.0553 - val_accuracy: 0.0536\n",
      "Epoch 7/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 272.2798 - accuracy: 0.0255 - val_loss: 143.0181 - val_accuracy: 0.0531\n",
      "Epoch 8/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 271.9047 - accuracy: 0.0257 - val_loss: 143.7594 - val_accuracy: 0.0550\n",
      "Epoch 9/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 269.5139 - accuracy: 0.0251 - val_loss: 142.4905 - val_accuracy: 0.0487\n",
      "Epoch 10/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 266.1370 - accuracy: 0.0262 - val_loss: 144.8623 - val_accuracy: 0.0454\n",
      "Epoch 11/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 263.6920 - accuracy: 0.0265 - val_loss: 139.6988 - val_accuracy: 0.0513\n",
      "Epoch 12/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 265.8222 - accuracy: 0.0258 - val_loss: 140.4291 - val_accuracy: 0.0450\n",
      "Epoch 13/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 263.2535 - accuracy: 0.0243 - val_loss: 151.5862 - val_accuracy: 0.0402\n",
      "Epoch 14/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 261.5061 - accuracy: 0.0258 - val_loss: 137.5877 - val_accuracy: 0.0548\n",
      "Epoch 15/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 256.1862 - accuracy: 0.0278 - val_loss: 142.2823 - val_accuracy: 0.0490\n",
      "Epoch 16/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 255.5649 - accuracy: 0.0272 - val_loss: 139.3359 - val_accuracy: 0.0499\n",
      "Epoch 17/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 255.0311 - accuracy: 0.0260 - val_loss: 140.7067 - val_accuracy: 0.0478\n",
      "Epoch 18/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 254.6250 - accuracy: 0.0248 - val_loss: 136.8955 - val_accuracy: 0.0519\n",
      "Epoch 19/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 251.7702 - accuracy: 0.0286 - val_loss: 138.5467 - val_accuracy: 0.0485\n",
      "Epoch 20/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 248.3112 - accuracy: 0.0278 - val_loss: 142.3047 - val_accuracy: 0.0427\n",
      "Epoch 21/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 248.6683 - accuracy: 0.0270 - val_loss: 138.0593 - val_accuracy: 0.0528\n",
      "Epoch 22/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 247.8329 - accuracy: 0.0286 - val_loss: 139.2503 - val_accuracy: 0.0473\n",
      "Epoch 23/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 243.1565 - accuracy: 0.0278 - val_loss: 142.6269 - val_accuracy: 0.0470\n",
      "Epoch 24/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 240.0712 - accuracy: 0.0279 - val_loss: 140.1064 - val_accuracy: 0.0454\n",
      "Epoch 25/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 237.1312 - accuracy: 0.0261 - val_loss: 135.4357 - val_accuracy: 0.0542\n",
      "Epoch 26/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 237.5132 - accuracy: 0.0281 - val_loss: 136.6712 - val_accuracy: 0.0539\n",
      "Epoch 27/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 236.0193 - accuracy: 0.0289 - val_loss: 137.1378 - val_accuracy: 0.0507\n",
      "Epoch 28/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 234.1554 - accuracy: 0.0288 - val_loss: 142.4720 - val_accuracy: 0.0447\n",
      "Epoch 29/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 229.4356 - accuracy: 0.0288 - val_loss: 137.7254 - val_accuracy: 0.0459\n",
      "Epoch 30/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 228.6014 - accuracy: 0.0281 - val_loss: 140.6002 - val_accuracy: 0.0427\n",
      "Epoch 31/1000000\n",
      "26048/26048 [==============================] - ETA: 0s - loss: 228.2011 - accuracy: 0.02 - 0s 9us/step - loss: 228.0078 - accuracy: 0.0284 - val_loss: 137.7566 - val_accuracy: 0.0494\n",
      "Epoch 32/1000000\n",
      "26048/26048 [==============================] - ETA: 0s - loss: 224.4182 - accuracy: 0.02 - 0s 9us/step - loss: 224.5384 - accuracy: 0.0288 - val_loss: 135.3588 - val_accuracy: 0.0524\n",
      "Epoch 33/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 223.0917 - accuracy: 0.0313 - val_loss: 137.2518 - val_accuracy: 0.0496\n",
      "Epoch 34/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 221.3730 - accuracy: 0.0300 - val_loss: 137.5856 - val_accuracy: 0.0478\n",
      "Epoch 35/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 221.4907 - accuracy: 0.0271 - val_loss: 137.1859 - val_accuracy: 0.0478\n",
      "Epoch 36/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 218.1048 - accuracy: 0.0292 - val_loss: 140.8743 - val_accuracy: 0.0428\n",
      "Epoch 37/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 217.5547 - accuracy: 0.0265 - val_loss: 135.6911 - val_accuracy: 0.0501\n",
      "Epoch 38/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 215.6359 - accuracy: 0.0298 - val_loss: 134.5327 - val_accuracy: 0.0544\n",
      "Epoch 39/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 214.8556 - accuracy: 0.0284 - val_loss: 135.5631 - val_accuracy: 0.0517\n",
      "Epoch 40/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 213.7488 - accuracy: 0.0288 - val_loss: 134.9563 - val_accuracy: 0.0513\n",
      "Epoch 41/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 210.2219 - accuracy: 0.0294 - val_loss: 131.8010 - val_accuracy: 0.0631\n",
      "Epoch 42/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 210.9226 - accuracy: 0.0297 - val_loss: 138.0997 - val_accuracy: 0.0473\n",
      "Epoch 43/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 209.3840 - accuracy: 0.0328 - val_loss: 137.1376 - val_accuracy: 0.0488\n",
      "Epoch 44/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 206.6417 - accuracy: 0.0327 - val_loss: 135.6848 - val_accuracy: 0.0504\n",
      "Epoch 45/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 205.1910 - accuracy: 0.0312 - val_loss: 137.3053 - val_accuracy: 0.0479\n",
      "Epoch 46/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 204.5138 - accuracy: 0.0321 - val_loss: 132.1011 - val_accuracy: 0.0568\n",
      "Epoch 47/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 202.5290 - accuracy: 0.0328 - val_loss: 134.5214 - val_accuracy: 0.0513\n",
      "Epoch 48/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 204.6653 - accuracy: 0.0296 - val_loss: 136.2324 - val_accuracy: 0.0504\n",
      "Epoch 49/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 201.9413 - accuracy: 0.0303 - val_loss: 135.5326 - val_accuracy: 0.0490\n",
      "Epoch 50/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 200.3719 - accuracy: 0.0324 - val_loss: 133.3518 - val_accuracy: 0.0516\n",
      "Epoch 51/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 200.4345 - accuracy: 0.0327 - val_loss: 136.3199 - val_accuracy: 0.0517\n",
      "Epoch 52/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26048/26048 [==============================] - 0s 9us/step - loss: 200.5586 - accuracy: 0.0324 - val_loss: 136.3280 - val_accuracy: 0.0510\n",
      "Epoch 53/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 196.9878 - accuracy: 0.0332 - val_loss: 133.3950 - val_accuracy: 0.0520\n",
      "Epoch 54/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 199.5545 - accuracy: 0.0319 - val_loss: 135.5194 - val_accuracy: 0.0491\n",
      "Epoch 55/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 195.9690 - accuracy: 0.0324 - val_loss: 135.8372 - val_accuracy: 0.0485\n",
      "Epoch 56/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 197.1600 - accuracy: 0.0347 - val_loss: 130.5084 - val_accuracy: 0.0608\n",
      "Epoch 57/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 195.2989 - accuracy: 0.0337 - val_loss: 134.4365 - val_accuracy: 0.0488\n",
      "Epoch 58/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 192.2296 - accuracy: 0.0325 - val_loss: 132.7619 - val_accuracy: 0.0484\n",
      "Epoch 59/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 193.1058 - accuracy: 0.0327 - val_loss: 133.5121 - val_accuracy: 0.0482\n",
      "Epoch 60/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 193.1276 - accuracy: 0.0349 - val_loss: 131.7631 - val_accuracy: 0.0527\n",
      "Epoch 61/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 193.2667 - accuracy: 0.0331 - val_loss: 131.1801 - val_accuracy: 0.0542\n",
      "Epoch 62/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 189.9650 - accuracy: 0.0331 - val_loss: 134.2958 - val_accuracy: 0.0444\n",
      "Epoch 63/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 188.0792 - accuracy: 0.0347 - val_loss: 134.8898 - val_accuracy: 0.0453\n",
      "Epoch 64/1000000\n",
      "26048/26048 [==============================] - ETA: 0s - loss: 188.8616 - accuracy: 0.03 - 0s 9us/step - loss: 188.7935 - accuracy: 0.0352 - val_loss: 131.0672 - val_accuracy: 0.0548\n",
      "Epoch 65/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 188.7297 - accuracy: 0.0329 - val_loss: 131.5258 - val_accuracy: 0.0539\n",
      "Epoch 66/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 188.9795 - accuracy: 0.0352 - val_loss: 131.6691 - val_accuracy: 0.0600\n",
      "Epoch 67/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 186.1508 - accuracy: 0.0348 - val_loss: 132.1858 - val_accuracy: 0.0553\n",
      "Epoch 68/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 185.4537 - accuracy: 0.0356 - val_loss: 132.5477 - val_accuracy: 0.0534\n",
      "Epoch 69/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 186.1248 - accuracy: 0.0351 - val_loss: 132.4348 - val_accuracy: 0.0545\n",
      "Epoch 70/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 183.7654 - accuracy: 0.0338 - val_loss: 132.4099 - val_accuracy: 0.0539\n",
      "Epoch 71/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 184.6147 - accuracy: 0.0355 - val_loss: 131.4021 - val_accuracy: 0.0605\n",
      "Epoch 72/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 182.6267 - accuracy: 0.0342 - val_loss: 133.9238 - val_accuracy: 0.0514\n",
      "Epoch 73/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 180.5198 - accuracy: 0.0357 - val_loss: 131.8837 - val_accuracy: 0.0550\n",
      "Epoch 74/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 181.3542 - accuracy: 0.0353 - val_loss: 130.2748 - val_accuracy: 0.0579\n",
      "Epoch 75/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 182.5054 - accuracy: 0.0365 - val_loss: 131.8856 - val_accuracy: 0.0573\n",
      "Epoch 76/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 179.9835 - accuracy: 0.0379 - val_loss: 129.5169 - val_accuracy: 0.0593\n",
      "Epoch 77/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 179.2792 - accuracy: 0.0362 - val_loss: 130.8294 - val_accuracy: 0.0563\n",
      "Epoch 78/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 182.2128 - accuracy: 0.0355 - val_loss: 132.0193 - val_accuracy: 0.0571\n",
      "Epoch 79/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 176.4316 - accuracy: 0.0365 - val_loss: 132.1590 - val_accuracy: 0.0537\n",
      "Epoch 80/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 177.8015 - accuracy: 0.0352 - val_loss: 129.4403 - val_accuracy: 0.0562\n",
      "Epoch 81/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 176.7529 - accuracy: 0.0364 - val_loss: 128.5608 - val_accuracy: 0.0582\n",
      "Epoch 82/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 176.6244 - accuracy: 0.0362 - val_loss: 133.9798 - val_accuracy: 0.0502\n",
      "Epoch 83/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 174.8562 - accuracy: 0.0354 - val_loss: 131.3638 - val_accuracy: 0.0611\n",
      "Epoch 84/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 175.2675 - accuracy: 0.0380 - val_loss: 129.3636 - val_accuracy: 0.0570\n",
      "Epoch 85/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 174.1315 - accuracy: 0.0370 - val_loss: 130.7241 - val_accuracy: 0.0588\n",
      "Epoch 86/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 174.9642 - accuracy: 0.0364 - val_loss: 129.0582 - val_accuracy: 0.0571\n",
      "Epoch 87/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 174.4004 - accuracy: 0.0365 - val_loss: 129.9697 - val_accuracy: 0.0622\n",
      "Epoch 88/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 173.9030 - accuracy: 0.0372 - val_loss: 131.5343 - val_accuracy: 0.0587\n",
      "Epoch 89/1000000\n",
      "26048/26048 [==============================] - ETA: 0s - loss: 173.2362 - accuracy: 0.03 - 0s 9us/step - loss: 172.6935 - accuracy: 0.0354 - val_loss: 130.8218 - val_accuracy: 0.0602\n",
      "Epoch 90/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 170.6525 - accuracy: 0.0355 - val_loss: 132.2803 - val_accuracy: 0.0554\n",
      "Epoch 91/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 172.2457 - accuracy: 0.0352 - val_loss: 131.2022 - val_accuracy: 0.0623\n",
      "Epoch 92/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 169.9645 - accuracy: 0.0349 - val_loss: 133.0804 - val_accuracy: 0.0544\n",
      "Epoch 93/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 170.5492 - accuracy: 0.0372 - val_loss: 130.6169 - val_accuracy: 0.0585\n",
      "Epoch 94/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 170.8787 - accuracy: 0.0377 - val_loss: 129.6425 - val_accuracy: 0.0622\n",
      "Epoch 95/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 168.9114 - accuracy: 0.0390 - val_loss: 129.0014 - val_accuracy: 0.0639\n",
      "Epoch 96/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 167.2857 - accuracy: 0.0375 - val_loss: 128.4571 - val_accuracy: 0.0594\n",
      "Epoch 97/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 169.2468 - accuracy: 0.0360 - val_loss: 131.4201 - val_accuracy: 0.0551\n",
      "Epoch 98/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 166.8430 - accuracy: 0.0382 - val_loss: 128.6355 - val_accuracy: 0.0591\n",
      "Epoch 99/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 167.0858 - accuracy: 0.0365 - val_loss: 129.5228 - val_accuracy: 0.0602\n",
      "Epoch 100/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 165.2871 - accuracy: 0.0375 - val_loss: 130.6696 - val_accuracy: 0.0554\n",
      "Epoch 101/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 165.1949 - accuracy: 0.0354 - val_loss: 130.5699 - val_accuracy: 0.0562\n",
      "Epoch 102/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 166.0161 - accuracy: 0.0387 - val_loss: 128.7447 - val_accuracy: 0.0617\n",
      "Epoch 103/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 165.9415 - accuracy: 0.0378 - val_loss: 130.2541 - val_accuracy: 0.0567\n",
      "Epoch 104/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26048/26048 [==============================] - 0s 9us/step - loss: 164.5974 - accuracy: 0.0352 - val_loss: 129.6258 - val_accuracy: 0.0622\n",
      "Epoch 105/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 161.6993 - accuracy: 0.0371 - val_loss: 127.4297 - val_accuracy: 0.0628\n",
      "Epoch 106/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 162.8197 - accuracy: 0.0352 - val_loss: 129.2022 - val_accuracy: 0.0636\n",
      "Epoch 107/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 162.7708 - accuracy: 0.0368 - val_loss: 129.4205 - val_accuracy: 0.0639\n",
      "Epoch 108/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 163.7750 - accuracy: 0.0382 - val_loss: 129.2687 - val_accuracy: 0.0628\n",
      "Epoch 109/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 161.1259 - accuracy: 0.0409 - val_loss: 129.5774 - val_accuracy: 0.0582\n",
      "Epoch 110/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 160.9180 - accuracy: 0.0390 - val_loss: 131.7650 - val_accuracy: 0.0556\n",
      "Epoch 111/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 162.1349 - accuracy: 0.0386 - val_loss: 128.6292 - val_accuracy: 0.0573\n",
      "Epoch 112/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 158.4290 - accuracy: 0.0385 - val_loss: 129.8555 - val_accuracy: 0.0560\n",
      "Epoch 113/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 159.5297 - accuracy: 0.0385 - val_loss: 130.7299 - val_accuracy: 0.0577\n",
      "Epoch 114/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 156.7903 - accuracy: 0.0377 - val_loss: 128.7362 - val_accuracy: 0.0613\n",
      "Epoch 115/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 157.0910 - accuracy: 0.0368 - val_loss: 130.5584 - val_accuracy: 0.0567\n",
      "Epoch 116/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 157.5056 - accuracy: 0.0374 - val_loss: 129.1395 - val_accuracy: 0.0619\n",
      "Epoch 117/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 158.7348 - accuracy: 0.0398 - val_loss: 131.2544 - val_accuracy: 0.0540\n",
      "Epoch 118/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 156.0092 - accuracy: 0.0398 - val_loss: 127.9453 - val_accuracy: 0.0580\n",
      "Epoch 119/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 156.7339 - accuracy: 0.0387 - val_loss: 128.3030 - val_accuracy: 0.0622\n",
      "Epoch 120/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 155.1133 - accuracy: 0.0394 - val_loss: 128.8328 - val_accuracy: 0.0619\n",
      "Epoch 121/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 155.3404 - accuracy: 0.0392 - val_loss: 129.0168 - val_accuracy: 0.0603\n",
      "Epoch 122/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 154.2970 - accuracy: 0.0403 - val_loss: 130.3575 - val_accuracy: 0.0568\n",
      "Epoch 123/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 155.5095 - accuracy: 0.0389 - val_loss: 128.0945 - val_accuracy: 0.0625\n",
      "Epoch 124/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 154.9915 - accuracy: 0.0402 - val_loss: 128.7688 - val_accuracy: 0.0622\n",
      "Epoch 125/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 153.7897 - accuracy: 0.0390 - val_loss: 129.2769 - val_accuracy: 0.0620\n",
      "Epoch 126/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 153.1765 - accuracy: 0.0397 - val_loss: 130.2775 - val_accuracy: 0.0585\n",
      "Epoch 127/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 152.2257 - accuracy: 0.0415 - val_loss: 128.3091 - val_accuracy: 0.0623\n",
      "Epoch 128/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 153.2780 - accuracy: 0.0398 - val_loss: 128.8536 - val_accuracy: 0.0616\n",
      "Epoch 129/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 153.5043 - accuracy: 0.0414 - val_loss: 127.5445 - val_accuracy: 0.0603\n",
      "Epoch 130/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 151.0496 - accuracy: 0.0403 - val_loss: 130.1919 - val_accuracy: 0.0559\n",
      "Epoch 131/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 151.8607 - accuracy: 0.0388 - val_loss: 130.0014 - val_accuracy: 0.0594\n",
      "Epoch 132/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 149.9348 - accuracy: 0.0410 - val_loss: 129.8002 - val_accuracy: 0.0605\n",
      "Epoch 133/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 151.1448 - accuracy: 0.0410 - val_loss: 129.1162 - val_accuracy: 0.0617\n",
      "Epoch 134/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 150.6136 - accuracy: 0.0402 - val_loss: 126.6704 - val_accuracy: 0.0623\n",
      "Epoch 135/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 149.5939 - accuracy: 0.0420 - val_loss: 128.9863 - val_accuracy: 0.0570\n",
      "Epoch 136/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 149.5478 - accuracy: 0.0422 - val_loss: 127.9188 - val_accuracy: 0.0654\n",
      "Epoch 137/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 148.0334 - accuracy: 0.0440 - val_loss: 128.6927 - val_accuracy: 0.0600\n",
      "Epoch 138/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 147.9867 - accuracy: 0.0411 - val_loss: 128.1053 - val_accuracy: 0.0643\n",
      "Epoch 139/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 147.5230 - accuracy: 0.0397 - val_loss: 128.4862 - val_accuracy: 0.0639\n",
      "Epoch 140/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 147.8668 - accuracy: 0.0416 - val_loss: 127.8589 - val_accuracy: 0.0633\n",
      "Epoch 141/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 146.2274 - accuracy: 0.0438 - val_loss: 127.7705 - val_accuracy: 0.0634\n",
      "Epoch 142/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 147.5525 - accuracy: 0.0410 - val_loss: 129.3378 - val_accuracy: 0.0582\n",
      "Epoch 143/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 146.6789 - accuracy: 0.0427 - val_loss: 129.5768 - val_accuracy: 0.0611\n",
      "Epoch 144/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 146.4870 - accuracy: 0.0411 - val_loss: 129.6963 - val_accuracy: 0.0587\n",
      "Epoch 145/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 146.5768 - accuracy: 0.0429 - val_loss: 128.3401 - val_accuracy: 0.0625\n",
      "Epoch 146/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 145.1756 - accuracy: 0.0437 - val_loss: 127.3440 - val_accuracy: 0.0616\n",
      "Epoch 147/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 146.2107 - accuracy: 0.0408 - val_loss: 128.3794 - val_accuracy: 0.0645\n",
      "Epoch 148/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 145.6314 - accuracy: 0.0423 - val_loss: 127.2842 - val_accuracy: 0.0659\n",
      "Epoch 149/1000000\n",
      "26048/26048 [==============================] - 0s 12us/step - loss: 143.4010 - accuracy: 0.0421 - val_loss: 126.9218 - val_accuracy: 0.0617\n",
      "Epoch 150/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 143.7659 - accuracy: 0.0438 - val_loss: 127.8711 - val_accuracy: 0.0648\n",
      "Epoch 151/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 144.3217 - accuracy: 0.0427 - val_loss: 127.3773 - val_accuracy: 0.0648\n",
      "Epoch 152/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 141.3799 - accuracy: 0.0427 - val_loss: 127.8746 - val_accuracy: 0.0626\n",
      "Epoch 153/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 142.4262 - accuracy: 0.0420 - val_loss: 127.6368 - val_accuracy: 0.0616\n",
      "Epoch 154/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 142.1481 - accuracy: 0.0441 - val_loss: 129.0603 - val_accuracy: 0.0617\n",
      "Epoch 155/1000000\n",
      "26048/26048 [==============================] - ETA: 0s - loss: 143.9754 - accuracy: 0.04 - 0s 9us/step - loss: 143.8242 - accuracy: 0.0436 - val_loss: 128.1190 - val_accuracy: 0.0630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 141.2545 - accuracy: 0.0472 - val_loss: 126.9296 - val_accuracy: 0.0640\n",
      "Epoch 157/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 141.6666 - accuracy: 0.0446 - val_loss: 129.2994 - val_accuracy: 0.0622\n",
      "Epoch 158/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 141.5836 - accuracy: 0.0422 - val_loss: 127.3409 - val_accuracy: 0.0637\n",
      "Epoch 159/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 140.6122 - accuracy: 0.0457 - val_loss: 128.8795 - val_accuracy: 0.0631\n",
      "Epoch 160/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 141.8453 - accuracy: 0.0453 - val_loss: 127.8351 - val_accuracy: 0.0669\n",
      "Epoch 161/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 141.7796 - accuracy: 0.0448 - val_loss: 128.4215 - val_accuracy: 0.0626\n",
      "Epoch 162/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 140.5509 - accuracy: 0.0469 - val_loss: 128.0613 - val_accuracy: 0.0628\n",
      "Epoch 163/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 139.6728 - accuracy: 0.0440 - val_loss: 128.4922 - val_accuracy: 0.0633\n",
      "Epoch 164/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 139.4699 - accuracy: 0.0465 - val_loss: 128.5392 - val_accuracy: 0.0633\n",
      "Epoch 165/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 139.6625 - accuracy: 0.0469 - val_loss: 127.2552 - val_accuracy: 0.0654\n",
      "Epoch 166/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 139.0558 - accuracy: 0.0478 - val_loss: 127.7753 - val_accuracy: 0.0656\n",
      "Epoch 167/1000000\n",
      "26048/26048 [==============================] - ETA: 0s - loss: 139.7223 - accuracy: 0.04 - 0s 9us/step - loss: 139.6783 - accuracy: 0.0471 - val_loss: 126.8827 - val_accuracy: 0.0648\n",
      "Epoch 168/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 139.1300 - accuracy: 0.0450 - val_loss: 129.0846 - val_accuracy: 0.0633\n",
      "Epoch 169/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 138.6662 - accuracy: 0.0442 - val_loss: 127.5473 - val_accuracy: 0.0657\n",
      "Epoch 170/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 137.3494 - accuracy: 0.0484 - val_loss: 126.7275 - val_accuracy: 0.0662\n",
      "Epoch 171/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 138.5903 - accuracy: 0.0458 - val_loss: 127.0834 - val_accuracy: 0.0631\n",
      "Epoch 172/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 138.2233 - accuracy: 0.0478 - val_loss: 127.7949 - val_accuracy: 0.0634\n",
      "Epoch 173/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 139.0540 - accuracy: 0.0471 - val_loss: 127.5125 - val_accuracy: 0.0669\n",
      "Epoch 174/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 136.5287 - accuracy: 0.0462 - val_loss: 127.0805 - val_accuracy: 0.0613\n",
      "Epoch 175/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 137.0896 - accuracy: 0.0445 - val_loss: 127.2701 - val_accuracy: 0.0623\n",
      "Epoch 176/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 136.9067 - accuracy: 0.0470 - val_loss: 128.1782 - val_accuracy: 0.0619\n",
      "Epoch 177/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 136.4840 - accuracy: 0.0469 - val_loss: 127.7852 - val_accuracy: 0.0637\n",
      "Epoch 178/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 135.3367 - accuracy: 0.0473 - val_loss: 129.2257 - val_accuracy: 0.0614\n",
      "Epoch 179/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 136.7104 - accuracy: 0.0461 - val_loss: 129.0372 - val_accuracy: 0.0633\n",
      "Epoch 180/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 135.5718 - accuracy: 0.0473 - val_loss: 126.6394 - val_accuracy: 0.0651\n",
      "Epoch 181/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 135.4444 - accuracy: 0.0456 - val_loss: 127.1363 - val_accuracy: 0.0659\n",
      "Epoch 182/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 135.9308 - accuracy: 0.0481 - val_loss: 127.2925 - val_accuracy: 0.0685\n",
      "Epoch 183/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 134.6300 - accuracy: 0.0482 - val_loss: 126.9595 - val_accuracy: 0.0651\n",
      "Epoch 184/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 134.7346 - accuracy: 0.0483 - val_loss: 126.7823 - val_accuracy: 0.0648\n",
      "Epoch 185/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 135.6570 - accuracy: 0.0482 - val_loss: 127.2950 - val_accuracy: 0.0648\n",
      "Epoch 186/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 134.6403 - accuracy: 0.0458 - val_loss: 127.2571 - val_accuracy: 0.0665\n",
      "Epoch 187/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 134.9922 - accuracy: 0.0488 - val_loss: 127.4107 - val_accuracy: 0.0679\n",
      "Epoch 188/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 134.0017 - accuracy: 0.0499 - val_loss: 127.4236 - val_accuracy: 0.0634\n",
      "Epoch 189/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 134.3701 - accuracy: 0.0488 - val_loss: 127.3499 - val_accuracy: 0.0679\n",
      "Epoch 190/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 133.0341 - accuracy: 0.0489 - val_loss: 127.3141 - val_accuracy: 0.0660\n",
      "Epoch 191/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 133.9067 - accuracy: 0.0453 - val_loss: 127.1462 - val_accuracy: 0.0656\n",
      "Epoch 192/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 133.6563 - accuracy: 0.0491 - val_loss: 126.6082 - val_accuracy: 0.0674\n",
      "Epoch 193/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 133.6281 - accuracy: 0.0497 - val_loss: 127.1969 - val_accuracy: 0.0654\n",
      "Epoch 194/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 133.1785 - accuracy: 0.0485 - val_loss: 126.9334 - val_accuracy: 0.0648\n",
      "Epoch 195/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 133.6467 - accuracy: 0.0495 - val_loss: 126.7043 - val_accuracy: 0.0642\n",
      "Epoch 196/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 133.3360 - accuracy: 0.0498 - val_loss: 127.6303 - val_accuracy: 0.0686\n",
      "Epoch 197/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 132.7194 - accuracy: 0.0475 - val_loss: 127.1660 - val_accuracy: 0.0649\n",
      "Epoch 198/1000000\n",
      "26048/26048 [==============================] - ETA: 0s - loss: 133.6610 - accuracy: 0.04 - 0s 9us/step - loss: 133.3523 - accuracy: 0.0495 - val_loss: 127.4970 - val_accuracy: 0.0674\n",
      "Epoch 199/1000000\n",
      "26048/26048 [==============================] - ETA: 0s - loss: 132.5195 - accuracy: 0.04 - 0s 9us/step - loss: 132.7297 - accuracy: 0.0499 - val_loss: 127.1342 - val_accuracy: 0.0680\n",
      "Epoch 200/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 132.9018 - accuracy: 0.0493 - val_loss: 126.9159 - val_accuracy: 0.0633\n",
      "Epoch 201/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 131.9532 - accuracy: 0.0483 - val_loss: 127.0454 - val_accuracy: 0.0643\n",
      "Epoch 202/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 132.1434 - accuracy: 0.0511 - val_loss: 126.8114 - val_accuracy: 0.0665\n",
      "Epoch 203/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 131.3991 - accuracy: 0.0489 - val_loss: 126.2367 - val_accuracy: 0.0625\n",
      "Epoch 204/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 131.5394 - accuracy: 0.0496 - val_loss: 126.9422 - val_accuracy: 0.0660\n",
      "Epoch 205/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 131.8307 - accuracy: 0.0508 - val_loss: 127.6472 - val_accuracy: 0.0637\n",
      "Epoch 206/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 132.3289 - accuracy: 0.0499 - val_loss: 126.3424 - val_accuracy: 0.0633\n",
      "Epoch 207/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26048/26048 [==============================] - 0s 11us/step - loss: 130.9961 - accuracy: 0.0501 - val_loss: 128.2222 - val_accuracy: 0.0645\n",
      "Epoch 208/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 131.3454 - accuracy: 0.0499 - val_loss: 127.2332 - val_accuracy: 0.0682\n",
      "Epoch 209/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 131.0661 - accuracy: 0.0517 - val_loss: 126.8651 - val_accuracy: 0.0654\n",
      "Epoch 210/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 131.6114 - accuracy: 0.0487 - val_loss: 126.9881 - val_accuracy: 0.0671\n",
      "Epoch 211/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 130.9881 - accuracy: 0.0517 - val_loss: 126.7759 - val_accuracy: 0.0643\n",
      "Epoch 212/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 130.7196 - accuracy: 0.0500 - val_loss: 126.6330 - val_accuracy: 0.0633\n",
      "Epoch 213/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 131.0079 - accuracy: 0.0538 - val_loss: 127.3541 - val_accuracy: 0.0663\n",
      "Epoch 214/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 130.1386 - accuracy: 0.0534 - val_loss: 126.8493 - val_accuracy: 0.0666\n",
      "Epoch 215/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 129.9717 - accuracy: 0.0519 - val_loss: 127.4949 - val_accuracy: 0.0668\n",
      "Epoch 216/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 130.5192 - accuracy: 0.0511 - val_loss: 126.8237 - val_accuracy: 0.0648\n",
      "Epoch 217/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 130.5293 - accuracy: 0.0503 - val_loss: 126.9632 - val_accuracy: 0.0663\n",
      "Epoch 218/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 129.5805 - accuracy: 0.0535 - val_loss: 128.2821 - val_accuracy: 0.0637\n",
      "Epoch 219/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 129.3870 - accuracy: 0.0537 - val_loss: 126.3581 - val_accuracy: 0.0642\n",
      "Epoch 220/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 129.4814 - accuracy: 0.0508 - val_loss: 126.4428 - val_accuracy: 0.0653\n",
      "Epoch 221/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 129.8117 - accuracy: 0.0526 - val_loss: 127.3118 - val_accuracy: 0.0653\n",
      "Epoch 222/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 128.8456 - accuracy: 0.0508 - val_loss: 127.6087 - val_accuracy: 0.0666\n",
      "Epoch 223/1000000\n",
      "26048/26048 [==============================] - 0s 12us/step - loss: 129.8936 - accuracy: 0.0516 - val_loss: 126.9801 - val_accuracy: 0.0651\n",
      "Epoch 224/1000000\n",
      "26048/26048 [==============================] - 0s 12us/step - loss: 128.8472 - accuracy: 0.0530 - val_loss: 126.6174 - val_accuracy: 0.0637\n",
      "Epoch 225/1000000\n",
      "26048/26048 [==============================] - 0s 13us/step - loss: 129.3788 - accuracy: 0.0534 - val_loss: 126.9346 - val_accuracy: 0.0657\n",
      "Epoch 226/1000000\n",
      "26048/26048 [==============================] - 0s 12us/step - loss: 129.0254 - accuracy: 0.0522 - val_loss: 127.2536 - val_accuracy: 0.0691\n",
      "Epoch 227/1000000\n",
      "26048/26048 [==============================] - 0s 12us/step - loss: 128.5659 - accuracy: 0.0514 - val_loss: 126.9124 - val_accuracy: 0.0666\n",
      "Epoch 228/1000000\n",
      "26048/26048 [==============================] - 0s 12us/step - loss: 129.0215 - accuracy: 0.0524 - val_loss: 126.7172 - val_accuracy: 0.0642\n",
      "Epoch 229/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 128.2125 - accuracy: 0.0513 - val_loss: 127.2171 - val_accuracy: 0.0663\n",
      "Epoch 230/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 128.3696 - accuracy: 0.0527 - val_loss: 127.0864 - val_accuracy: 0.0669\n",
      "Epoch 231/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 128.6329 - accuracy: 0.0535 - val_loss: 126.9372 - val_accuracy: 0.0646\n",
      "Epoch 232/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 128.2429 - accuracy: 0.0542 - val_loss: 126.9826 - val_accuracy: 0.0646\n",
      "Epoch 233/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 128.0824 - accuracy: 0.0524 - val_loss: 126.8803 - val_accuracy: 0.0651\n",
      "Epoch 234/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 128.6507 - accuracy: 0.0508 - val_loss: 126.9638 - val_accuracy: 0.0663\n",
      "Epoch 235/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 128.6803 - accuracy: 0.0533 - val_loss: 127.5408 - val_accuracy: 0.0668\n",
      "Epoch 236/1000000\n",
      "26048/26048 [==============================] - 0s 12us/step - loss: 128.0223 - accuracy: 0.0549 - val_loss: 127.0886 - val_accuracy: 0.0669\n",
      "Epoch 237/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 128.1657 - accuracy: 0.0531 - val_loss: 126.9435 - val_accuracy: 0.0648\n",
      "Epoch 238/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 128.1589 - accuracy: 0.0520 - val_loss: 127.4811 - val_accuracy: 0.0657\n",
      "Epoch 239/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 128.0198 - accuracy: 0.0525 - val_loss: 126.9412 - val_accuracy: 0.0665\n",
      "Epoch 240/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 128.6037 - accuracy: 0.0522 - val_loss: 127.1222 - val_accuracy: 0.0665\n",
      "Epoch 241/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.1058 - accuracy: 0.0535 - val_loss: 127.1696 - val_accuracy: 0.0649\n",
      "Epoch 242/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.8796 - accuracy: 0.0531 - val_loss: 127.1903 - val_accuracy: 0.0656\n",
      "Epoch 243/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 128.0353 - accuracy: 0.0522 - val_loss: 126.5674 - val_accuracy: 0.0639\n",
      "Epoch 244/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.7171 - accuracy: 0.0532 - val_loss: 127.3378 - val_accuracy: 0.0673\n",
      "Epoch 245/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.7130 - accuracy: 0.0529 - val_loss: 126.8278 - val_accuracy: 0.0643\n",
      "Epoch 246/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.6526 - accuracy: 0.0543 - val_loss: 127.2421 - val_accuracy: 0.0639\n",
      "Epoch 247/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.4818 - accuracy: 0.0555 - val_loss: 126.6527 - val_accuracy: 0.0622\n",
      "Epoch 248/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 128.2770 - accuracy: 0.0538 - val_loss: 127.4332 - val_accuracy: 0.0657\n",
      "Epoch 249/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.5860 - accuracy: 0.0536 - val_loss: 126.8845 - val_accuracy: 0.0636\n",
      "Epoch 250/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.1561 - accuracy: 0.0540 - val_loss: 126.5556 - val_accuracy: 0.0651\n",
      "Epoch 251/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.3831 - accuracy: 0.0543 - val_loss: 126.8181 - val_accuracy: 0.0669\n",
      "Epoch 252/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.8219 - accuracy: 0.0537 - val_loss: 126.9978 - val_accuracy: 0.0648\n",
      "Epoch 253/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.0429 - accuracy: 0.0559 - val_loss: 126.8266 - val_accuracy: 0.0649\n",
      "Epoch 254/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.0973 - accuracy: 0.0555 - val_loss: 126.8127 - val_accuracy: 0.0643\n",
      "Epoch 255/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.4310 - accuracy: 0.0564 - val_loss: 127.0728 - val_accuracy: 0.0643\n",
      "Epoch 256/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.2479 - accuracy: 0.0552 - val_loss: 127.3841 - val_accuracy: 0.0668\n",
      "Epoch 257/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.1469 - accuracy: 0.0558 - val_loss: 126.7263 - val_accuracy: 0.0639\n",
      "Epoch 258/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.6914 - accuracy: 0.0549 - val_loss: 127.5817 - val_accuracy: 0.0665\n",
      "Epoch 259/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.4189 - accuracy: 0.0569 - val_loss: 127.0886 - val_accuracy: 0.0648\n",
      "Epoch 260/1000000\n",
      "26048/26048 [==============================] - 0s 16us/step - loss: 126.5966 - accuracy: 0.0565 - val_loss: 127.4467 - val_accuracy: 0.0645\n",
      "Epoch 261/1000000\n",
      "26048/26048 [==============================] - 0s 15us/step - loss: 127.0585 - accuracy: 0.0531 - val_loss: 127.2424 - val_accuracy: 0.0671\n",
      "Epoch 262/1000000\n",
      "26048/26048 [==============================] - 0s 13us/step - loss: 126.9312 - accuracy: 0.0540 - val_loss: 127.7742 - val_accuracy: 0.0656\n",
      "Epoch 263/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 126.7684 - accuracy: 0.0550 - val_loss: 127.0559 - val_accuracy: 0.0663\n",
      "Epoch 264/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 127.1887 - accuracy: 0.0537 - val_loss: 127.2190 - val_accuracy: 0.0645\n",
      "Epoch 265/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 127.6458 - accuracy: 0.0577 - val_loss: 127.0438 - val_accuracy: 0.0634\n",
      "Epoch 266/1000000\n",
      "26048/26048 [==============================] - 0s 15us/step - loss: 127.3755 - accuracy: 0.0559 - val_loss: 127.5597 - val_accuracy: 0.0634\n",
      "Epoch 267/1000000\n",
      "26048/26048 [==============================] - 0s 15us/step - loss: 127.0883 - accuracy: 0.0539 - val_loss: 127.5740 - val_accuracy: 0.0634\n",
      "Epoch 268/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 126.3651 - accuracy: 0.0568 - val_loss: 127.2009 - val_accuracy: 0.0656\n",
      "Epoch 269/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 127.9369 - accuracy: 0.0588 - val_loss: 127.3575 - val_accuracy: 0.0673\n",
      "Epoch 270/1000000\n",
      "26048/26048 [==============================] - 0s 16us/step - loss: 126.9000 - accuracy: 0.0576 - val_loss: 127.8687 - val_accuracy: 0.0662\n",
      "Epoch 271/1000000\n",
      "26048/26048 [==============================] - 0s 15us/step - loss: 127.0443 - accuracy: 0.0589 - val_loss: 128.2352 - val_accuracy: 0.0663\n",
      "Epoch 272/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 127.5160 - accuracy: 0.0579 - val_loss: 127.3551 - val_accuracy: 0.0666\n",
      "Epoch 273/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 127.4873 - accuracy: 0.0570 - val_loss: 127.8990 - val_accuracy: 0.0686\n",
      "Epoch 274/1000000\n",
      "26048/26048 [==============================] - 0s 12us/step - loss: 127.1906 - accuracy: 0.0588 - val_loss: 127.4296 - val_accuracy: 0.0654\n",
      "Epoch 275/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.9082 - accuracy: 0.0559 - val_loss: 127.5868 - val_accuracy: 0.0654\n",
      "Epoch 276/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 127.7186 - accuracy: 0.0551 - val_loss: 127.5500 - val_accuracy: 0.0657\n",
      "Epoch 277/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 127.0771 - accuracy: 0.0584 - val_loss: 127.6247 - val_accuracy: 0.0630\n",
      "Epoch 278/1000000\n",
      "26048/26048 [==============================] - 0s 13us/step - loss: 127.0483 - accuracy: 0.0574 - val_loss: 127.8072 - val_accuracy: 0.0643\n",
      "Epoch 279/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 127.0868 - accuracy: 0.0571 - val_loss: 127.3812 - val_accuracy: 0.0671\n",
      "Epoch 280/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 127.2330 - accuracy: 0.0557 - val_loss: 127.4026 - val_accuracy: 0.0649\n",
      "Epoch 281/1000000\n",
      "26048/26048 [==============================] - 0s 15us/step - loss: 126.7831 - accuracy: 0.0553 - val_loss: 128.0564 - val_accuracy: 0.0643\n",
      "Epoch 282/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 127.0344 - accuracy: 0.0564 - val_loss: 127.2900 - val_accuracy: 0.0616\n",
      "Epoch 283/1000000\n",
      "26048/26048 [==============================] - 0s 17us/step - loss: 126.8960 - accuracy: 0.0570 - val_loss: 127.6500 - val_accuracy: 0.0663\n",
      "Epoch 284/1000000\n",
      "26048/26048 [==============================] - 0s 16us/step - loss: 127.0062 - accuracy: 0.0567 - val_loss: 127.6865 - val_accuracy: 0.0653\n",
      "Epoch 285/1000000\n",
      "26048/26048 [==============================] - 0s 16us/step - loss: 127.4868 - accuracy: 0.0564 - val_loss: 127.9125 - val_accuracy: 0.0668\n",
      "Epoch 286/1000000\n",
      "26048/26048 [==============================] - 0s 17us/step - loss: 127.6700 - accuracy: 0.0565 - val_loss: 127.8445 - val_accuracy: 0.0683\n",
      "Epoch 287/1000000\n",
      "26048/26048 [==============================] - 0s 16us/step - loss: 126.9959 - accuracy: 0.0561 - val_loss: 127.9373 - val_accuracy: 0.0676\n",
      "Epoch 288/1000000\n",
      "26048/26048 [==============================] - 0s 15us/step - loss: 126.7527 - accuracy: 0.0601 - val_loss: 127.9438 - val_accuracy: 0.0674\n",
      "Epoch 289/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 126.7561 - accuracy: 0.0554 - val_loss: 128.3553 - val_accuracy: 0.0669\n",
      "Epoch 290/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 127.0642 - accuracy: 0.0584 - val_loss: 127.8042 - val_accuracy: 0.0646\n",
      "Epoch 291/1000000\n",
      "26048/26048 [==============================] - 0s 12us/step - loss: 126.7894 - accuracy: 0.0591 - val_loss: 127.4453 - val_accuracy: 0.0626\n",
      "Epoch 292/1000000\n",
      "26048/26048 [==============================] - 0s 16us/step - loss: 126.0276 - accuracy: 0.0577 - val_loss: 128.0584 - val_accuracy: 0.0677\n",
      "Epoch 293/1000000\n",
      "26048/26048 [==============================] - 0s 16us/step - loss: 127.2247 - accuracy: 0.0592 - val_loss: 127.8142 - val_accuracy: 0.0688\n",
      "Epoch 294/1000000\n",
      "26048/26048 [==============================] - 0s 12us/step - loss: 127.0665 - accuracy: 0.0597 - val_loss: 127.4218 - val_accuracy: 0.0642\n",
      "Epoch 295/1000000\n",
      "26048/26048 [==============================] - 0s 14us/step - loss: 126.7603 - accuracy: 0.0572 - val_loss: 127.6109 - val_accuracy: 0.0639\n",
      "Epoch 296/1000000\n",
      "26048/26048 [==============================] - 0s 17us/step - loss: 126.6763 - accuracy: 0.0597 - val_loss: 128.8552 - val_accuracy: 0.0692\n",
      "Epoch 297/1000000\n",
      "26048/26048 [==============================] - 0s 13us/step - loss: 127.1266 - accuracy: 0.0613 - val_loss: 128.2129 - val_accuracy: 0.0682\n",
      "Epoch 298/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 126.6255 - accuracy: 0.0589 - val_loss: 128.8853 - val_accuracy: 0.0680\n",
      "Epoch 299/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.0064 - accuracy: 0.0599 - val_loss: 128.1745 - val_accuracy: 0.0680\n",
      "Epoch 300/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.3651 - accuracy: 0.0608 - val_loss: 127.8233 - val_accuracy: 0.0656\n",
      "Epoch 301/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.3316 - accuracy: 0.0575 - val_loss: 128.0385 - val_accuracy: 0.0711\n",
      "Epoch 302/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.4990 - accuracy: 0.0575 - val_loss: 127.6190 - val_accuracy: 0.0622\n",
      "Epoch 303/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.0353 - accuracy: 0.0590 - val_loss: 128.1578 - val_accuracy: 0.0689\n",
      "Epoch 304/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.9114 - accuracy: 0.0591 - val_loss: 127.9352 - val_accuracy: 0.0680\n",
      "Epoch 305/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.4753 - accuracy: 0.0593 - val_loss: 127.9054 - val_accuracy: 0.0666\n",
      "Epoch 306/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.0045 - accuracy: 0.0595 - val_loss: 127.8192 - val_accuracy: 0.0666\n",
      "Epoch 307/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.0501 - accuracy: 0.0575 - val_loss: 128.0909 - val_accuracy: 0.0666\n",
      "Epoch 308/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.6136 - accuracy: 0.0570 - val_loss: 128.0867 - val_accuracy: 0.0699\n",
      "Epoch 309/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.5334 - accuracy: 0.0589 - val_loss: 127.8185 - val_accuracy: 0.0656\n",
      "Epoch 310/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.8813 - accuracy: 0.0571 - val_loss: 128.9695 - val_accuracy: 0.0668\n",
      "Epoch 311/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.2453 - accuracy: 0.0594 - val_loss: 128.3805 - val_accuracy: 0.0697\n",
      "Epoch 312/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.9143 - accuracy: 0.0586 - val_loss: 127.8728 - val_accuracy: 0.0669\n",
      "Epoch 313/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.8051 - accuracy: 0.0607 - val_loss: 127.7386 - val_accuracy: 0.0648\n",
      "Epoch 314/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.6560 - accuracy: 0.0586 - val_loss: 127.8776 - val_accuracy: 0.0654\n",
      "Epoch 315/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.1891 - accuracy: 0.0580 - val_loss: 127.8983 - val_accuracy: 0.0685\n",
      "Epoch 316/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.0075 - accuracy: 0.0584 - val_loss: 128.5138 - val_accuracy: 0.0689\n",
      "Epoch 317/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.9254 - accuracy: 0.0578 - val_loss: 128.1059 - val_accuracy: 0.0706\n",
      "Epoch 318/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.9795 - accuracy: 0.0610 - val_loss: 128.0215 - val_accuracy: 0.0685\n",
      "Epoch 319/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.6454 - accuracy: 0.0588 - val_loss: 127.9869 - val_accuracy: 0.0671\n",
      "Epoch 320/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.0872 - accuracy: 0.0592 - val_loss: 127.8950 - val_accuracy: 0.0676\n",
      "Epoch 321/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.9445 - accuracy: 0.0572 - val_loss: 128.3594 - val_accuracy: 0.0696\n",
      "Epoch 322/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.1699 - accuracy: 0.0595 - val_loss: 127.7745 - val_accuracy: 0.0649\n",
      "Epoch 323/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.7499 - accuracy: 0.0597 - val_loss: 128.4650 - val_accuracy: 0.0708\n",
      "Epoch 324/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.4755 - accuracy: 0.0588 - val_loss: 127.8492 - val_accuracy: 0.0653\n",
      "Epoch 325/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.8039 - accuracy: 0.0593 - val_loss: 127.9067 - val_accuracy: 0.0669\n",
      "Epoch 326/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.5386 - accuracy: 0.0594 - val_loss: 127.6526 - val_accuracy: 0.0680\n",
      "Epoch 327/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.6638 - accuracy: 0.0591 - val_loss: 127.8293 - val_accuracy: 0.0648\n",
      "Epoch 328/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.4300 - accuracy: 0.0594 - val_loss: 128.3590 - val_accuracy: 0.0682\n",
      "Epoch 329/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.6978 - accuracy: 0.0599 - val_loss: 127.6368 - val_accuracy: 0.0648\n",
      "Epoch 330/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.0575 - accuracy: 0.0588 - val_loss: 128.1312 - val_accuracy: 0.0677\n",
      "Epoch 331/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.7722 - accuracy: 0.0595 - val_loss: 127.6784 - val_accuracy: 0.0686\n",
      "Epoch 332/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.5949 - accuracy: 0.0610 - val_loss: 127.8939 - val_accuracy: 0.0668\n",
      "Epoch 333/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.8241 - accuracy: 0.0581 - val_loss: 128.1822 - val_accuracy: 0.0680\n",
      "Epoch 334/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 125.9014 - accuracy: 0.0592 - val_loss: 128.4877 - val_accuracy: 0.0669\n",
      "Epoch 335/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 125.8181 - accuracy: 0.0613 - val_loss: 127.9921 - val_accuracy: 0.0673\n",
      "Epoch 336/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.5527 - accuracy: 0.0612 - val_loss: 127.9704 - val_accuracy: 0.0663\n",
      "Epoch 337/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.8603 - accuracy: 0.0580 - val_loss: 128.6704 - val_accuracy: 0.0685\n",
      "Epoch 338/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.5034 - accuracy: 0.0586 - val_loss: 128.2094 - val_accuracy: 0.0686\n",
      "Epoch 339/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.7004 - accuracy: 0.0602 - val_loss: 127.9423 - val_accuracy: 0.0679\n",
      "Epoch 340/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 125.8905 - accuracy: 0.0592 - val_loss: 127.8779 - val_accuracy: 0.0679\n",
      "Epoch 341/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.3964 - accuracy: 0.0605 - val_loss: 128.5347 - val_accuracy: 0.0712\n",
      "Epoch 342/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.5642 - accuracy: 0.0617 - val_loss: 127.8350 - val_accuracy: 0.0700\n",
      "Epoch 343/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.5442 - accuracy: 0.0592 - val_loss: 128.0177 - val_accuracy: 0.0668\n",
      "Epoch 344/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.1249 - accuracy: 0.0601 - val_loss: 128.0107 - val_accuracy: 0.0669\n",
      "Epoch 345/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.6280 - accuracy: 0.0617 - val_loss: 128.8784 - val_accuracy: 0.0674\n",
      "Epoch 346/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.3198 - accuracy: 0.0568 - val_loss: 127.9110 - val_accuracy: 0.0666\n",
      "Epoch 347/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.4804 - accuracy: 0.0584 - val_loss: 127.5421 - val_accuracy: 0.0662\n",
      "Epoch 348/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.1242 - accuracy: 0.0571 - val_loss: 128.1742 - val_accuracy: 0.0653\n",
      "Epoch 349/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.1716 - accuracy: 0.0625 - val_loss: 127.9946 - val_accuracy: 0.0654\n",
      "Epoch 350/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.5649 - accuracy: 0.0579 - val_loss: 128.6055 - val_accuracy: 0.0669\n",
      "Epoch 351/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.0058 - accuracy: 0.0576 - val_loss: 128.0203 - val_accuracy: 0.0683\n",
      "Epoch 352/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.6905 - accuracy: 0.0597 - val_loss: 128.4859 - val_accuracy: 0.0685\n",
      "Epoch 353/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.9569 - accuracy: 0.0587 - val_loss: 127.8415 - val_accuracy: 0.0705\n",
      "Epoch 354/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.3684 - accuracy: 0.0592 - val_loss: 127.8146 - val_accuracy: 0.0663\n",
      "Epoch 355/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.4735 - accuracy: 0.0590 - val_loss: 127.9656 - val_accuracy: 0.0685\n",
      "Epoch 356/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.5991 - accuracy: 0.0606 - val_loss: 128.0640 - val_accuracy: 0.0660\n",
      "Epoch 357/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.5656 - accuracy: 0.0597 - val_loss: 127.6211 - val_accuracy: 0.0686\n",
      "Epoch 358/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.6752 - accuracy: 0.0607 - val_loss: 127.9943 - val_accuracy: 0.0676\n",
      "Epoch 359/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 127.3079 - accuracy: 0.0594 - val_loss: 128.1162 - val_accuracy: 0.0666\n",
      "Epoch 360/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.7533 - accuracy: 0.0598 - val_loss: 128.3979 - val_accuracy: 0.0657\n",
      "Epoch 361/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 125.7828 - accuracy: 0.0598 - val_loss: 127.9518 - val_accuracy: 0.0674\n",
      "Epoch 362/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.0552 - accuracy: 0.0613 - val_loss: 128.8349 - val_accuracy: 0.0665\n",
      "Epoch 363/1000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26048/26048 [==============================] - 0s 11us/step - loss: 126.8184 - accuracy: 0.0608 - val_loss: 128.1006 - val_accuracy: 0.0677\n",
      "Epoch 364/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.4443 - accuracy: 0.0593 - val_loss: 128.0670 - val_accuracy: 0.0674\n",
      "Epoch 365/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.2172 - accuracy: 0.0574 - val_loss: 127.7651 - val_accuracy: 0.0686\n",
      "Epoch 366/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 126.6970 - accuracy: 0.0572 - val_loss: 128.0962 - val_accuracy: 0.0666\n",
      "Epoch 367/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 126.8855 - accuracy: 0.0570 - val_loss: 128.0892 - val_accuracy: 0.0662\n",
      "Epoch 368/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.7864 - accuracy: 0.0593 - val_loss: 128.1999 - val_accuracy: 0.0692\n",
      "Epoch 369/1000000\n",
      "26048/26048 [==============================] - 0s 11us/step - loss: 126.6939 - accuracy: 0.0587 - val_loss: 128.1688 - val_accuracy: 0.0697\n",
      "Epoch 370/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.3665 - accuracy: 0.0606 - val_loss: 127.7491 - val_accuracy: 0.0679\n",
      "Epoch 371/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.8967 - accuracy: 0.0611 - val_loss: 128.2873 - val_accuracy: 0.0691\n",
      "Epoch 372/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.8827 - accuracy: 0.0570 - val_loss: 128.2447 - val_accuracy: 0.0692\n",
      "Epoch 373/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 125.7660 - accuracy: 0.0587 - val_loss: 128.4546 - val_accuracy: 0.0679\n",
      "Epoch 374/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.1792 - accuracy: 0.0588 - val_loss: 128.1223 - val_accuracy: 0.0697\n",
      "Epoch 375/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.6295 - accuracy: 0.0594 - val_loss: 128.2809 - val_accuracy: 0.0685\n",
      "Epoch 376/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.6639 - accuracy: 0.0617 - val_loss: 127.9887 - val_accuracy: 0.0699\n",
      "Epoch 377/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.5994 - accuracy: 0.0577 - val_loss: 128.3265 - val_accuracy: 0.0700\n",
      "Epoch 378/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.1037 - accuracy: 0.0600 - val_loss: 127.8360 - val_accuracy: 0.0697\n",
      "Epoch 379/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.3589 - accuracy: 0.0596 - val_loss: 128.5772 - val_accuracy: 0.0677\n",
      "Epoch 380/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.5898 - accuracy: 0.0587 - val_loss: 128.0988 - val_accuracy: 0.0691\n",
      "Epoch 381/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.7582 - accuracy: 0.0605 - val_loss: 128.1543 - val_accuracy: 0.0705\n",
      "Epoch 382/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.9842 - accuracy: 0.0610 - val_loss: 128.1336 - val_accuracy: 0.0683\n",
      "Epoch 383/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.5575 - accuracy: 0.0601 - val_loss: 127.9970 - val_accuracy: 0.0686\n",
      "Epoch 384/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.7563 - accuracy: 0.0574 - val_loss: 128.1672 - val_accuracy: 0.0680\n",
      "Epoch 385/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.4793 - accuracy: 0.0608 - val_loss: 128.3278 - val_accuracy: 0.0685\n",
      "Epoch 386/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.1549 - accuracy: 0.0621 - val_loss: 127.9402 - val_accuracy: 0.0671\n",
      "Epoch 387/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.1444 - accuracy: 0.0598 - val_loss: 127.7715 - val_accuracy: 0.0643\n",
      "Epoch 388/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.4006 - accuracy: 0.0577 - val_loss: 128.4493 - val_accuracy: 0.0679\n",
      "Epoch 389/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.3711 - accuracy: 0.0591 - val_loss: 128.3206 - val_accuracy: 0.0697\n",
      "Epoch 390/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.0532 - accuracy: 0.0595 - val_loss: 128.4807 - val_accuracy: 0.0699\n",
      "Epoch 391/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.5118 - accuracy: 0.0591 - val_loss: 128.0053 - val_accuracy: 0.0663\n",
      "Epoch 392/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.0585 - accuracy: 0.0612 - val_loss: 128.4113 - val_accuracy: 0.0683\n",
      "Epoch 393/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 126.3573 - accuracy: 0.0615 - val_loss: 128.0073 - val_accuracy: 0.0677\n",
      "Epoch 394/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.4332 - accuracy: 0.0602 - val_loss: 128.0322 - val_accuracy: 0.0676\n",
      "Epoch 395/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.4236 - accuracy: 0.0569 - val_loss: 128.4931 - val_accuracy: 0.0692\n",
      "Epoch 396/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 127.2163 - accuracy: 0.0599 - val_loss: 128.5438 - val_accuracy: 0.0694\n",
      "Epoch 397/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 125.7869 - accuracy: 0.0590 - val_loss: 128.4792 - val_accuracy: 0.0688\n",
      "Epoch 398/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.4817 - accuracy: 0.0575 - val_loss: 128.2597 - val_accuracy: 0.0702\n",
      "Epoch 399/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.3399 - accuracy: 0.0623 - val_loss: 128.4019 - val_accuracy: 0.0685\n",
      "Epoch 400/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 125.8121 - accuracy: 0.0602 - val_loss: 128.1830 - val_accuracy: 0.0709\n",
      "Epoch 401/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 125.8472 - accuracy: 0.0592 - val_loss: 128.0815 - val_accuracy: 0.0705\n",
      "Epoch 402/1000000\n",
      "26048/26048 [==============================] - 0s 10us/step - loss: 125.9519 - accuracy: 0.0579 - val_loss: 127.8254 - val_accuracy: 0.0677\n",
      "Epoch 403/1000000\n",
      "26048/26048 [==============================] - 0s 9us/step - loss: 126.3705 - accuracy: 0.0617 - val_loss: 128.4929 - val_accuracy: 0.0700\n",
      "16281/16281 [==============================] - 0s 11us/step\n",
      "Test Acc : 0.05773601308465004\n",
      "Test Loss : 125.64718858882767\n",
      "MSE為： 125.64718823868125\n",
      "RMSE為： 11.209245658771211\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense , Dropout\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#dense指最普通的全連接層型態\n",
    "#dropout減少節點 通常設0.2~0.5 最高不會超過0.5\n",
    "#2層(最後一層ouput不算)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=200,restore_best_weights=True)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, input_shape=(X_train1.shape[1],), activation=\"relu\")) \n",
    "model.add(layers.Dropout(0.30)) #d砍上一層的節點\n",
    "model.add(layers.Dense(8, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.30))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train1, y_train1, validation_split=0.2, epochs=1000000, batch_size=128,callbacks=[early_stopping])\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test1, y_test1)\n",
    "print(\"Test Acc : \" + str(accuracy))\n",
    "print(\"Test Loss : \" + str(loss))\n",
    "y_pred1 = model.predict(X_test1)\n",
    "print('MSE為：',mean_squared_error(y_test1,y_pred1))\n",
    "#print('MSE為(直接计算)：',np.mean((y_test-y_pred)**2))\n",
    "print('RMSE為：',np.sqrt(mean_squared_error(y_test1,y_pred1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5f9ae0ee1ecc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense , Dropout\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#dense指最普通的全連接層型態\n",
    "#dropout減少節點 通常設0.2~0.5 最高不會超過0.5\n",
    "#2層(最後一層ouput不算)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=200,restore_best_weights=True)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, input_shape=(X_train.shape[1],), activation=\"relu\")) \n",
    "model.add(layers.Dropout(0.30)) #d砍上一層的節點\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.30))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.30))\n",
    "model.add(layers.Dense(8, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.30))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000000, batch_size=128,callbacks=[early_stopping])\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Acc : \" + str(accuracy))\n",
    "print(\"Test Loss : \" + str(loss))\n",
    "y_pred = model.predict(X_test)\n",
    "print('MSE為：',mean_squared_error(y_test,y_pred))\n",
    "#print('MSE為(直接计算)：',np.mean((y_test-y_pred)**2))\n",
    "print('RMSE為：',np.sqrt(mean_squared_error(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
